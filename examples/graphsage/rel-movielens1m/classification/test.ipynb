{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../../../../rllm/dataloader\")\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "# import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# from utils import load_data\n",
    "\n",
    "from load_data import load_data\n",
    "from models import GraphSage\n",
    "from utils import adj_matrix_to_list, multihop_sampling\n",
    "\n",
    "t_total = time.time()\n",
    "# Load data\n",
    "data, adj, features, labels, idx_train, idx_val, idx_test = load_data('movielens-classification')\n",
    "adjacency_dict = adj_matrix_to_list(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro: 0.14021669853409816; macro: 0.12027760362574674\n",
      "Epoch: 0001 loss_train: 4.6243 time: 2.1052s\n",
      "Epoch: 0002 loss_train: 4.2922 time: 0.1563s\n",
      "Epoch: 0003 loss_train: 4.4186 time: 0.1482s\n",
      "Epoch: 0004 loss_train: 4.2951 time: 0.1540s\n",
      "Epoch: 0005 loss_train: 4.2336 time: 0.1419s\n",
      "micro: 0.34491927825261154; macro: 0.0866862588833352\n",
      "Epoch: 0006 loss_train: 4.0365 time: 1.5356s\n",
      "Epoch: 0007 loss_train: 4.2655 time: 0.1463s\n",
      "Epoch: 0008 loss_train: 4.1566 time: 0.1471s\n",
      "Epoch: 0009 loss_train: 4.1169 time: 0.1508s\n",
      "Epoch: 0010 loss_train: 4.1048 time: 0.1417s\n",
      "micro: 0.36170818505338076; macro: 0.09107239325297393\n",
      "Epoch: 0011 loss_train: 4.1050 time: 1.6162s\n",
      "Epoch: 0012 loss_train: 3.9615 time: 0.1529s\n",
      "Epoch: 0013 loss_train: 4.1881 time: 0.1491s\n",
      "Epoch: 0014 loss_train: 4.0347 time: 0.1476s\n",
      "Epoch: 0015 loss_train: 3.9477 time: 0.1477s\n",
      "micro: 0.3420436570705473; macro: 0.1036681317880227\n",
      "Epoch: 0016 loss_train: 3.9941 time: 1.6117s\n",
      "Epoch: 0017 loss_train: 4.1377 time: 0.1506s\n",
      "Epoch: 0018 loss_train: 3.9511 time: 0.1462s\n",
      "Epoch: 0019 loss_train: 3.9591 time: 0.1492s\n",
      "Epoch: 0020 loss_train: 3.7868 time: 0.1428s\n",
      "micro: 0.35611510791366907; macro: 0.10071261581784825\n",
      "Epoch: 0021 loss_train: 3.8597 time: 1.6075s\n",
      "Epoch: 0022 loss_train: 3.9529 time: 0.1539s\n",
      "Epoch: 0023 loss_train: 4.1047 time: 0.1514s\n",
      "Epoch: 0024 loss_train: 4.1521 time: 0.2334s\n",
      "Epoch: 0025 loss_train: 3.8381 time: 0.1452s\n",
      "micro: 0.3416624171341152; macro: 0.11106686082948672\n",
      "Epoch: 0026 loss_train: 3.9641 time: 1.5967s\n",
      "Epoch: 0027 loss_train: 4.0884 time: 0.1473s\n",
      "Epoch: 0028 loss_train: 3.7171 time: 0.1428s\n",
      "Epoch: 0029 loss_train: 3.8665 time: 0.1475s\n",
      "Epoch: 0030 loss_train: 3.9585 time: 0.1495s\n",
      "micro: 0.35096378309877946; macro: 0.11015592726930909\n",
      "Epoch: 0031 loss_train: 3.7625 time: 1.6251s\n",
      "Epoch: 0032 loss_train: 3.6792 time: 0.1440s\n",
      "Epoch: 0033 loss_train: 4.0090 time: 0.1488s\n",
      "Epoch: 0034 loss_train: 3.8451 time: 0.1545s\n",
      "Epoch: 0035 loss_train: 3.9771 time: 0.1462s\n",
      "micro: 0.3511928040672664; macro: 0.11438991664231042\n",
      "Epoch: 0036 loss_train: 3.9755 time: 1.6247s\n",
      "Epoch: 0037 loss_train: 4.0142 time: 0.1517s\n",
      "Epoch: 0038 loss_train: 3.9337 time: 0.1468s\n",
      "Epoch: 0039 loss_train: 3.9872 time: 0.1440s\n",
      "Epoch: 0040 loss_train: 3.7362 time: 0.1427s\n",
      "micro: 0.35031635031635033; macro: 0.11057221187456433\n",
      "Epoch: 0041 loss_train: 3.9086 time: 1.6270s\n",
      "Epoch: 0042 loss_train: 3.8678 time: 0.1536s\n",
      "Epoch: 0043 loss_train: 3.6619 time: 0.1444s\n",
      "Epoch: 0044 loss_train: 3.7771 time: 0.1477s\n",
      "Epoch: 0045 loss_train: 3.6879 time: 0.1478s\n",
      "micro: 0.3555004818945339; macro: 0.10880498214790275\n",
      "Epoch: 0046 loss_train: 3.8008 time: 1.6362s\n",
      "Epoch: 0047 loss_train: 3.7569 time: 0.1594s\n",
      "Epoch: 0048 loss_train: 3.7431 time: 0.1449s\n",
      "Epoch: 0049 loss_train: 3.7298 time: 0.1519s\n",
      "Epoch: 0050 loss_train: 3.7947 time: 0.1520s\n",
      "micro: 0.3466938983916262; macro: 0.11509698655058756\n",
      "Epoch: 0051 loss_train: 3.8893 time: 1.6249s\n",
      "Epoch: 0052 loss_train: 3.7446 time: 0.1471s\n",
      "Epoch: 0053 loss_train: 3.7485 time: 0.1420s\n",
      "Epoch: 0054 loss_train: 3.8809 time: 0.1449s\n",
      "Epoch: 0055 loss_train: 3.7444 time: 0.1445s\n",
      "micro: 0.35149572649572647; macro: 0.11327636021634935\n",
      "Epoch: 0056 loss_train: 3.7881 time: 1.6117s\n",
      "Epoch: 0057 loss_train: 3.5912 time: 0.1470s\n",
      "Epoch: 0058 loss_train: 3.8317 time: 0.1473s\n",
      "Epoch: 0059 loss_train: 3.7568 time: 0.2374s\n",
      "Epoch: 0060 loss_train: 3.5713 time: 0.1491s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 27.1111s\n",
      "micro: 0.3534788540245566; macro: 0.11157572830912812\n"
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "#                     help='Disables CUDA training.')\n",
    "# parser.add_argument('--fastmode', action='store_true', default=False,\n",
    "#                     help='Validate during training pass.')\n",
    "# parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "# parser.add_argument('--epochs', type=int, default=50,\n",
    "#                     help='Number of epochs to train.')\n",
    "# parser.add_argument('--lr', type=float, default=0.01,\n",
    "#                     help='Initial learning rate.')\n",
    "# parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "#                     help='Weight decay (L2 loss on parameters).')\n",
    "# parser.add_argument('--hidden', type=list, default=[128, 18],\n",
    "#                     help='Number of hidden units.')\n",
    "# parser.add_argument('--dropout', type=float, default=0.5,\n",
    "#                     help='Dropout rate (1 - keep probability).')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args_cuda = True\n",
    "args_hidden = [64, 18]\n",
    "args_lr = 0.005\n",
    "args_weight_decay = 5e-4\n",
    "args_epochs = 60\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if args_cuda:\n",
    "    torch.cuda.manual_seed(42)\n",
    "# Model and optimizer\n",
    "\n",
    "NUM_NEIGHBORS_LIST = [25, 10]\n",
    "NUM_BATCH_PER_EPOCH = 5\n",
    "batch_size = 64\n",
    "model = GraphSage(input_dim=features.shape[1], hidden_dim=args_hidden,\n",
    "                  num_neighbors_list=NUM_NEIGHBORS_LIST)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args_lr, weight_decay=args_weight_decay)\n",
    "# loss_func = nn.BCEWithLogitsLoss()\n",
    "loss_func = F.cross_entropy\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "if args_cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    loss_lst = []\n",
    "    if epoch % 5 == 0: test()\n",
    "    for batch in range(NUM_BATCH_PER_EPOCH):\n",
    "        optimizer.zero_grad()\n",
    "        batch_src_index = idx_train[torch.randint(0, len(idx_train), (batch_size,))]\n",
    "        batch_src_label = labels[batch_src_index].float().to(DEVICE)\n",
    "        batch_sampling_result = multihop_sampling(batch_src_index, NUM_NEIGHBORS_LIST, adjacency_dict)\n",
    "        batch_sampling_x = [features[idx].float().to(DEVICE) for idx in batch_sampling_result]\n",
    "        output = model(batch_sampling_x)\n",
    "        loss_train = loss_func(torch.sigmoid(output), batch_src_label)\n",
    "        loss_lst.append(loss_train.detach().item())\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "    # if not args.fastmode:\n",
    "    #     # Evaluate validation set performance separately,\n",
    "    #     # deactivates dropout during validation run.\n",
    "    #     model.eval()\n",
    "    #     output = model(features, adj)\n",
    "\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "            'loss_train: {:.4f}'.format(sum(loss_lst)/len(loss_lst)),\n",
    "        #   'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "        #   'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "            'time: {:.4f}s'.format(time.time() - t))\n",
    "    # print('acc_train: {:.4f}'.format(acc_train.item()/num))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_sampling_result = multihop_sampling(idx_test, NUM_NEIGHBORS_LIST, adjacency_dict)\n",
    "        test_x = [features[idx].float().to(DEVICE) for idx in test_sampling_result]\n",
    "        test_logits = model(test_x)\n",
    "        test_logits = torch.sigmoid(test_logits).cpu()\n",
    "        pred = np.where(test_logits > 0.5, 1, 0)\n",
    "        test_label = labels[idx_test].float().cpu()\n",
    "        f1_micro_test = f1_score(test_label, pred, average=\"micro\")\n",
    "        f1_macro_test = f1_score(test_label, pred, average=\"macro\")\n",
    "        print(f\"micro: {f1_micro_test}; macro: {f1_macro_test}\")\n",
    "\n",
    "\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(args_epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
